{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mlrg.hmc import HMCSampler\n",
    "from rgflow import RGLayer, RGPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "class GaussianModel(torch.nn.Module):\n",
    "    ''' phi4 model energy\n",
    "        E = (1/2) sum_<ij> |x_i-x_j|^2 + (r/2) sum_i |x_i|^2\n",
    "\n",
    "        Parameters:\n",
    "            r :: real - (initial) value of\n",
    "            '''\n",
    "    def __init__(self, r=0.):\n",
    "        super().__init__()\n",
    "        self.r = torch.nn.Parameter(torch.tensor(r))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'r={self.r.item()}'\n",
    "\n",
    "    def return_param(self):\n",
    "        return self.r\n",
    "\n",
    "    def clone(self):\n",
    "        mdl = type(self)()\n",
    "        mdl.load_state_dict(self.state_dict())\n",
    "        return mdl\n",
    "\n",
    "    def forward(self, x):\n",
    "        energy = 0.\n",
    "        for axis in range(2, x.dim()):\n",
    "            dx2 = (x.roll(1,axis) - x).square().sum(1)\n",
    "            energy = energy + dx2 / 2\n",
    "        x2 = x.square().sum(1)\n",
    "        energy = energy + self.r * x2 / 2\n",
    "        energy = energy.view(energy.shape[:1]+(-1,)).sum(-1)\n",
    "        return energy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([4.3420e-01, 1.6956e-01, 3.8666e-01, 2.4564e-01, 5.3521e-01, 5.6790e-02,\n",
      "        8.3578e-04, 5.8300e-02, 9.8736e-01, 1.7811e-01],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "hmc = HMCSampler(GaussianModel(r=3.), [1,1,2])\n",
    "x = hmc.sample(device, samples=10).detach()\n",
    "print(x.dim())\n",
    "print(GaussianModel(r=3.)(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True])\n",
      "tensor([[[0.4042]]])\n"
     ]
    }
   ],
   "source": [
    "x, z = RGPartition([2]).split(x)\n",
    "print(RGPartition([2]).mask)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "class RGLearner(torch.nn.Module):\n",
    "    def __init__(self, uv_model, ir_model, device, uv_shape, dim, base_dist='Normal', **kwargs):\n",
    "        super().__init__()\n",
    "        self.uv_model = uv_model.requires_grad_(False)\n",
    "        # self.ir_model = uv_model.clone().requires_grad_(True)\n",
    "        self.ir_model = ir_model.requires_grad_(True)\n",
    "        self.rglayer = RGLayer(uv_shape, device, dim, **kwargs)\n",
    "        ir_shape = self.rglayer.partitioner.out_shape\n",
    "        self.ir_sampler = HMCSampler(self.ir_model, (1, dim)+torch.Size(ir_shape))\n",
    "        self.base_dist = getattr(torch.distributions, base_dist)(0., 1.)\n",
    "        self.ir_param = self.ir_model.return_param()\n",
    "        self.uv_param = self.uv_model.return_param()\n",
    "        self.inv_K = torch.tensor([[2 + self.uv_param, -2],[-2, 2 + self.uv_param]])\n",
    "        self.K = torch.inverse(self.inv_K)\n",
    "\n",
    "    def check_ir_param(self):\n",
    "        return self.ir_param.requires_grad\n",
    "\n",
    "    def sample(self, samples, device):\n",
    "        with torch.no_grad():\n",
    "            return self.rsample(samples, device)\n",
    "\n",
    "    def rsample(self, samples, device):\n",
    "        x_ir = self.ir_sampler.sample(device, samples=samples)\n",
    "        print(x_ir.is_cuda)\n",
    "        z = self.base_dist.rsample(x_ir.shape[:2]+self.rglayer.partitioner.res_shape).to(device)\n",
    "        print(z.is_cuda)\n",
    "        x_uv, *_ = self.rglayer.decode(x_ir, z)\n",
    "        return x_uv\n",
    "\n",
    "    def loss(self, samples, device, lk=0.01, lg=0.01, mode=None, **kwargs):\n",
    "        x_ir = self.ir_sampler.sample(device, samples=samples, **kwargs)\n",
    "        z = self.base_dist.rsample(x_ir.shape[:2]+self.rglayer.partitioner.res_shape).to(device)\n",
    "        x_uv, logJ, Ek, Eg = self.rglayer.decode(x_ir, z, mode='jf_reg', **kwargs)\n",
    "        diff = self.uv_model(x_uv) - 1/2 * torch.log(1 / self.ir_param) - self.ir_model(x_ir) - logJ + 1 / 2 * torch.log(torch.det(self.K)) # The global minimum would be 0.5\n",
    "        Loss = diff + lk * Ek + lg * Eg      # original Loss\n",
    "        Loss_ = Loss.mean().detach()         # take average and detach\n",
    "        Loss = (1 + self.ir_model(x_ir).detach() - self.ir_model(x_ir)) * (Loss-Loss_) # implement the reinforce algorithm. log(p(x_ir)) = -E(x_ir) + const\n",
    "        Loss, diff, Ek, Eg = [val.mean() for val in (Loss, diff, Ek, Eg)]\n",
    "        return Loss, diff, Ek, Eg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "rgl = RGLearner(GaussianModel(r=1.).to(device), GaussianModel(r=5.).to(device), device, [2], 1, hdims=[8,8], hyper_dim=16)\n",
    "optimizer = torch.optim.Adam(rgl.parameters(), lr=0.001)\n",
    "print(rgl.check_ir_param())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.6239622858147413e-08 1.6502635478973389 1.005913496017456 0.755322277545929\n",
      "2.479553273815327e-08 1.3593195676803589 0.724684476852417 0.5482970476150513\n",
      "1.23977656585339e-07 1.2210677862167358 0.5099406838417053 0.5050588250160217\n",
      "-3.6239622858147413e-08 1.1664806604385376 0.4050995409488678 0.437779039144516\n",
      "-9.536743617033494e-10 1.0510467290878296 0.3214738070964813 0.38977086544036865\n",
      "3.910064805268121e-08 0.9596055746078491 0.2588021755218506 0.4024229943752289\n",
      "-1.907348723406699e-09 0.7425617575645447 0.20915080606937408 0.3447747826576233\n",
      "-9.53674295089968e-09 0.7503708600997925 0.1861545741558075 0.390712708234787\n",
      "1.525878978725359e-08 0.7415147423744202 0.18060027062892914 0.4217604994773865\n",
      "-4.482269133632144e-08 0.6209588646888733 0.18910381197929382 0.3638084828853607\n",
      "3.814697446813398e-09 0.7445825934410095 0.23300479352474213 0.37127551436424255\n",
      "-2.1457672971791908e-08 0.6792638301849365 0.271903395652771 0.41504931449890137\n",
      "1.9550324026340604e-08 0.6365292072296143 0.29845184087753296 0.48592913150787354\n",
      "-3.814697180359872e-08 0.6973174810409546 0.3332553803920746 0.47625860571861267\n",
      "2.0980834847250662e-08 0.6375211477279663 0.33888062834739685 0.4672456681728363\n",
      "3.814697446813398e-09 0.548237681388855 0.33898964524269104 0.5186752080917358\n",
      "-3.814697180359872e-08 0.49239397048950195 0.3066762685775757 0.520197868347168\n",
      "-5.626678500902926e-08 0.570295512676239 0.29741182923316956 0.5003831386566162\n",
      "-4.720687840631399e-08 0.5627809762954712 0.29489636421203613 0.49835553765296936\n",
      "4.577636758540393e-08 0.5114849209785461 0.27818524837493896 0.47239845991134644\n",
      "3.814697180359872e-08 0.6495292782783508 0.2876226305961609 0.48318079113960266\n",
      "-3.1709671333146616e-08 0.595124363899231 0.2797915041446686 0.455077588558197\n",
      "-4.100799699813251e-08 0.5789973139762878 0.2527546286582947 0.4516381025314331\n",
      "7.820129610536242e-08 0.564765989780426 0.2620256841182709 0.43260887265205383\n",
      "8.392333938900265e-08 0.5861635804176331 0.24883854389190674 0.4521389901638031\n",
      "2.3365020140886372e-08 0.544062614440918 0.2721814215183258 0.4033445715904236\n",
      "-6.103515914901436e-08 0.5426998138427734 0.2708457410335541 0.4304194152355194\n",
      "-3.051757957450718e-08 0.5877454876899719 0.282335102558136 0.48339590430259705\n",
      "-1.287460360543946e-08 0.5780484676361084 0.29189446568489075 0.48640871047973633\n",
      "7.915496524901755e-08 0.5323454141616821 0.2866423726081848 0.47443848848342896\n",
      "-5.2452087118126656e-09 0.5745092630386353 0.30028408765792847 0.4997639060020447\n",
      "1.9431114495205293e-08 0.5359365344047546 0.30734783411026 0.5474885106086731\n",
      "-1.931190496406998e-08 0.5423604249954224 0.3366313576698303 0.506209671497345\n",
      "1.1444091896350983e-08 0.5388466715812683 0.32244619727134705 0.5128618478775024\n",
      "4.577636758540393e-08 0.5573198795318604 0.3259568512439728 0.5221974849700928\n",
      "-3.6239622858147413e-08 0.5973584651947021 0.3307028114795685 0.49753445386886597\n",
      "2.8610228852699038e-08 0.5301291346549988 0.32384130358695984 0.4788792133331299\n",
      "2.6226043559063328e-08 0.5058007836341858 0.323899507522583 0.5212293267250061\n",
      "1.3351440841802287e-08 0.4828643202781677 0.3041042685508728 0.49120771884918213\n",
      "-2.574920721087892e-08 0.5189283490180969 0.3175612986087799 0.487876296043396\n",
      "2.574920721087892e-08 0.47939255833625793 0.2953806221485138 0.4628134071826935\n",
      "-6.1988831845383174e-09 0.5198875665664673 0.31098058819770813 0.4818416237831116\n",
      "-8.583068478174027e-09 0.4955059289932251 0.3031661808490753 0.44980114698410034\n",
      "3.767013723177115e-08 0.4659523069858551 0.3030613362789154 0.5055049061775208\n",
      "-1.0490417423625331e-08 0.565395176410675 0.3118777573108673 0.5256760120391846\n",
      "-4.577636758540393e-08 0.5541932582855225 0.32331719994544983 0.4668161869049072\n",
      "-2.813339250451463e-08 0.5629915595054626 0.3205778896808624 0.4926699101924896\n",
      "4.6730040281772744e-08 0.5233475565910339 0.31317830085754395 0.4735864996910095\n",
      "-7.915496524901755e-08 0.5453758835792542 0.31100380420684814 0.48985692858695984\n",
      "0.0 0.5067265629768372 0.29679858684539795 0.4580121338367462\n",
      "-9.53674295089968e-09 0.5052813291549683 0.2981622815132141 0.4561406970024109\n",
      "-6.413459630039142e-08 0.5204680562019348 0.3023074269294739 0.4528679847717285\n",
      "2.8610228852699038e-08 0.5605221390724182 0.30430907011032104 0.4685753583908081\n",
      "4.100799699813251e-08 0.512802004814148 0.2941646873950958 0.4816923439502716\n",
      "1.1920929132713809e-08 0.6059057116508484 0.3173128068447113 0.4812626838684082\n",
      "9.536743617033494e-10 0.5067909359931946 0.3026304543018341 0.45989757776260376\n",
      "1.907348723406699e-09 0.5418521761894226 0.30595076084136963 0.45142385363578796\n",
      "7.367133747493426e-08 0.5294185280799866 0.32169416546821594 0.4296943247318268\n",
      "2.8610229740877458e-09 0.49073705077171326 0.30891725420951843 0.4741719663143158\n",
      "3.051757957450718e-08 0.5447377562522888 0.3269651234149933 0.47787749767303467\n",
      "1.7166136956348055e-08 0.5687835812568665 0.3327486217021942 0.49871134757995605\n",
      "1.7166136956348055e-08 0.5235369205474854 0.3324778974056244 0.5678554177284241\n",
      "1.907348723406699e-09 0.4915076792240143 0.33935460448265076 0.4833805561065674\n",
      "-3.33786012163273e-08 0.5068389177322388 0.3413527309894562 0.5194999575614929\n",
      "3.1471252270875993e-08 0.46866723895072937 0.33603399991989136 0.48550719022750854\n",
      "-7.629394893626795e-09 0.442730188369751 0.3358265459537506 0.5177155137062073\n",
      "-1.2159347306806012e-08 0.5309363007545471 0.3504960238933563 0.4850586950778961\n",
      "2.8610228852699038e-08 0.5181354880332947 0.34890931844711304 0.5054409503936768\n",
      "9.53674295089968e-09 0.514880895614624 0.34401735663414 0.49286890029907227\n",
      "6.58035261835721e-08 0.5433799028396606 0.3404420018196106 0.4687304198741913\n",
      "3.7193299107229905e-08 0.5456485748291016 0.327093243598938 0.4582919180393219\n",
      "-7.247924571629483e-08 0.5352569818496704 0.33014988899230957 0.471220463514328\n",
      "4.005432074905002e-08 0.5400118827819824 0.33041664958000183 0.5072000622749329\n",
      "4.768371808516747e-10 0.45360058546066284 0.33200326561927795 0.5067605376243591\n",
      "-1.6689301052252858e-09 0.5660625100135803 0.35888388752937317 0.5280128121376038\n",
      "2.2888183792701966e-08 0.5090251564979553 0.3484189808368683 0.5247739553451538\n",
      "-4.291534239087014e-09 0.5752204656600952 0.37411731481552124 0.542425274848938\n",
      "-5.2452087118126656e-08 0.4789874255657196 0.35353395342826843 0.5081468820571899\n",
      "-9.536743617033494e-10 0.5197228789329529 0.3410758674144745 0.48680540919303894\n",
      "-5.2452087118126656e-08 0.528110146522522 0.3504016697406769 0.5102511048316956\n",
      "5.626678500902926e-08 0.5474702715873718 0.33390095829963684 0.4589969217777252\n",
      "1.525878978725359e-08 0.5169886350631714 0.31633466482162476 0.4638969898223877\n",
      "3.910064805268121e-08 0.5031790137290955 0.33329010009765625 0.46411728858947754\n",
      "9.536743617033494e-10 0.5090019702911377 0.3219892382621765 0.5151365399360657\n",
      "2.2888183792701966e-08 0.49630850553512573 0.32673323154449463 0.5020685791969299\n",
      "-2.2888183792701966e-08 0.48737287521362305 0.32967162132263184 0.5088623762130737\n",
      "-1.8119811429073707e-08 0.48279374837875366 0.34483614563941956 0.45230287313461304\n",
      "1.76429750808893e-08 0.5102676749229431 0.38077983260154724 0.5557394623756409\n",
      "5.3882597939036714e-08 0.5078068971633911 0.38280513882637024 0.5356787443161011\n",
      "9.53674295089968e-09 0.48821401596069336 0.3857823312282562 0.532463550567627\n",
      "-9.53674295089968e-09 0.6267775893211365 0.3947280943393707 0.518349289894104\n",
      "1.692771967043427e-08 0.5264360308647156 0.3649708926677704 0.5128659009933472\n",
      "1.3351440841802287e-08 0.4828157126903534 0.3429192304611206 0.4773152768611908\n",
      "-1.525878978725359e-08 0.46514251828193665 0.3658066987991333 0.533413290977478\n",
      "-2.002716037452501e-08 0.5033844113349915 0.3534986674785614 0.5424719452857971\n",
      "5.626678500902926e-08 0.513617992401123 0.3718830347061157 0.4916535019874573\n",
      "3.814697446813398e-09 0.46984782814979553 0.3499051630496979 0.4957796633243561\n",
      "-3.814697446813398e-09 0.46339845657348633 0.34288620948791504 0.4937831461429596\n",
      "5.7220459481754915e-09 0.47996166348457336 0.3329700231552124 0.48819610476493835\n",
      "-8.583068478174027e-09 0.4997978210449219 0.35338294506073 0.45708367228507996\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        loss, *rest = rgl.loss(1000, device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'{loss.item()} '+' '.join(f'{r.item()}' for r in rest))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "GaussianModel(r=4.96737813949585)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgl.ir_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
