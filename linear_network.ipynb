{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mlrg.hmc import HMCSampler\n",
    "from rgflow import RGLayer, RGPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "class GaussianModel(torch.nn.Module):\n",
    "    ''' phi4 model energy\n",
    "        E = (1/2) sum_<ij> |x_i-x_j|^2 + (r/2) sum_i |x_i|^2\n",
    "\n",
    "        Parameters:\n",
    "            r :: real - (initial) value of\n",
    "            '''\n",
    "    def __init__(self, r=0.):\n",
    "        super().__init__()\n",
    "        self.r = torch.nn.Parameter(torch.tensor(r))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'r={self.r.item()}'\n",
    "\n",
    "    def return_param(self):\n",
    "        return torch.exp(self.r)\n",
    "\n",
    "    def clone(self):\n",
    "        mdl = type(self)()\n",
    "        mdl.load_state_dict(self.state_dict())\n",
    "        return mdl\n",
    "\n",
    "    def forward(self, x):\n",
    "        energy = 0.\n",
    "        for axis in range(1, x.dim()):\n",
    "            dx2 = (x.roll(1,axis) - x).square().sum(1)\n",
    "            energy = energy + dx2 / 2\n",
    "        x2 = x.square().sum(1)\n",
    "        energy = energy + torch.exp(self.r) * x2 / 2\n",
    "        energy = energy.view(energy.shape[:1]+(-1,)).sum(-1)\n",
    "        return energy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "class LinearRGLearner(torch.nn.Module):\n",
    "    def __init__(self, uv_model, ir_model, device, base_dist='Normal'):\n",
    "        super().__init__()\n",
    "        self.uv_model = uv_model.requires_grad_(False)\n",
    "        # self.ir_model = uv_model.clone().requires_grad_(True)\n",
    "        self.ir_model = ir_model.requires_grad_(True)\n",
    "        self.ir_sampler = HMCSampler(self.ir_model, [1, 1])\n",
    "        self.base_dist = getattr(torch.distributions, base_dist)(0., 1.)\n",
    "        self.ir_param = self.ir_model.return_param()\n",
    "        self.uv_param = self.uv_model.return_param()\n",
    "        self.mat = torch.nn.Parameter(torch.randn(2, 2, requires_grad=True)).to(device)\n",
    "        self.inv_K = torch.tensor([[2 + self.uv_param, -2],[-2, 2 + self.uv_param]])\n",
    "        self.K = torch.inverse(self.inv_K)\n",
    "        self.G = torch.tensor([[1 / self.ir_param, 0],[0, 1]])\n",
    "\n",
    "\n",
    "    def get_mat(self):\n",
    "        return self.mat\n",
    "\n",
    "    def get_ir_model(self):\n",
    "        return self.ir_model\n",
    "\n",
    "    def sample(self, samples, device):\n",
    "        with torch.no_grad():\n",
    "            return self.rsample(samples, device)\n",
    "\n",
    "    def rsample(self, samples, device):\n",
    "        x_ir = self.ir_sampler.sample(device, samples=samples)\n",
    "        z = self.base_dist.rsample([samples, 1]).to(device)\n",
    "        return x_ir, z\n",
    "\n",
    "    def loss(self, samples, device, **kwargs):\n",
    "        x_ir = self.ir_sampler.sample(device, samples=samples, **kwargs)\n",
    "        z = self.base_dist.rsample([samples, 1]).to(device)\n",
    "        X_ir = torch.cat((x_ir, z), dim=1)\n",
    "        # X_ir.requires_grad_(True)\n",
    "        x_uv = torch.matmul(X_ir, torch.transpose(self.mat, 0, 1))\n",
    "        diff = self.uv_model(x_uv) - 1/2 * torch.log(1 / self.ir_param) - self.ir_model(x_ir) - torch.log(torch.abs(torch.det(self.mat))) + 1 / 2 * torch.log(torch.det(self.K))\n",
    "        Loss = diff    # original Loss\n",
    "        Loss = Loss.mean()\n",
    "        return Loss, self.uv_model(x_uv).mean()\n",
    "\n",
    "    def exact_loss(self, device):\n",
    "        M = torch.matmul(self.mat, self.G)\n",
    "        M = torch.matmul(self.inv_K, M)\n",
    "        Loss = 1 / 2 * torch.trace(torch.matmul(torch.transpose(self.mat, 0, 1), M)) + 1 / 2 * torch.log(torch.det(self.K)) - 1 / 2 * torch.log(1 / self.ir_param) - torch.log(torch.abs(torch.det(self.mat))) - 1\n",
    "        return Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Parameter containing:\n",
      "tensor([[-0.9880,  0.8620],\n",
      "        [-0.1869, -1.3567]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "rgl = LinearRGLearner(GaussianModel(r=1.).to(device), GaussianModel(r=1.).to(device), device)\n",
    "optimizer = torch.optim.Adam(rgl.parameters(), lr=0.005)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "print(sum(p.numel() for p in rgl.parameters() if p.requires_grad))\n",
    "print(rgl.get_mat())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47716858983039856 0.9642670154571533\n",
      "0.5018495321273804 0.979393720626831\n",
      "0.4955291152000427 0.936644434928894\n",
      "0.5053295493125916 0.9772124290466309\n",
      "0.48238471150398254 1.0085561275482178\n",
      "0.4935527443885803 0.9233084917068481\n",
      "0.4697396755218506 0.9764220714569092\n",
      "0.4868219494819641 0.9753586053848267\n",
      "0.4667348563671112 0.9457579255104065\n",
      "0.5308179259300232 1.0378681421279907\n",
      "0.5059708952903748 1.015560507774353\n",
      "0.5040596723556519 1.019698977470398\n",
      "0.49876639246940613 1.0003248453140259\n",
      "0.5193162560462952 0.9988695383071899\n",
      "0.4925503730773926 1.004390835762024\n",
      "0.4837201237678528 1.0002065896987915\n",
      "0.4695059657096863 0.9592027068138123\n",
      "0.46217477321624756 0.9419547915458679\n",
      "0.4840201735496521 0.9970679879188538\n",
      "0.5062101483345032 0.9927817583084106\n",
      "0.4933454394340515 0.9530473947525024\n",
      "0.4966079294681549 0.962620735168457\n",
      "0.4736888110637665 1.0142223834991455\n",
      "0.4948866069316864 1.0256555080413818\n",
      "0.4876980483531952 0.9675761461257935\n",
      "0.5136464834213257 1.0378751754760742\n",
      "0.4522152543067932 0.9418096542358398\n",
      "0.48515021800994873 0.9964839816093445\n",
      "0.4969411790370941 1.0087014436721802\n",
      "0.4525144398212433 0.957791268825531\n",
      "0.4772540330886841 0.9788098335266113\n",
      "0.4350944757461548 0.9522020816802979\n",
      "0.45152708888053894 0.9559376239776611\n",
      "0.47981780767440796 0.971348226070404\n",
      "0.5248525738716125 1.007002353668213\n",
      "0.4573991894721985 0.9781960844993591\n",
      "0.4666743278503418 0.9841417670249939\n",
      "0.4691096842288971 0.9893413782119751\n",
      "0.4895625710487366 1.0393321514129639\n",
      "0.4951549768447876 1.0018104314804077\n",
      "0.4903908669948578 1.016989827156067\n",
      "0.5018873810768127 1.0142072439193726\n",
      "0.5004781484603882 1.0070184469223022\n",
      "0.49945953488349915 1.027226209640503\n",
      "0.4821871519088745 1.0154544115066528\n",
      "0.510235607624054 1.0457067489624023\n",
      "0.42751213908195496 0.9616389870643616\n",
      "0.4900668263435364 0.995497465133667\n",
      "0.46131429076194763 0.9500123858451843\n",
      "0.5069548487663269 1.0379432439804077\n",
      "0.4881858229637146 0.9812692999839783\n",
      "0.47864392399787903 0.9892686605453491\n",
      "0.4899401068687439 1.0009558200836182\n",
      "0.490409791469574 1.0147985219955444\n",
      "0.48951709270477295 1.0051474571228027\n",
      "0.4907670021057129 0.9879173636436462\n",
      "0.45048725605010986 0.9059833884239197\n",
      "0.47075289487838745 0.9845259189605713\n",
      "0.48577770590782166 0.9813780784606934\n",
      "0.4608677923679352 0.980724036693573\n",
      "0.5225249528884888 1.02449369430542\n",
      "0.5279136896133423 1.0737494230270386\n",
      "0.5166286826133728 1.0171527862548828\n",
      "0.48142504692077637 1.0109589099884033\n",
      "0.526104748249054 1.0413472652435303\n",
      "0.46282607316970825 0.9540776014328003\n",
      "0.47724640369415283 1.0053019523620605\n",
      "0.46255260705947876 0.96434485912323\n",
      "0.5059857964515686 1.0281338691711426\n",
      "0.4809160828590393 0.9951686859130859\n",
      "0.4545031189918518 0.9597521424293518\n",
      "0.4891155958175659 1.037107229232788\n",
      "0.45657506585121155 0.9509772658348083\n",
      "0.45727792382240295 0.9991331696510315\n",
      "0.5063185095787048 1.056598424911499\n",
      "0.48326045274734497 0.9484896063804626\n",
      "0.47576436400413513 1.0240843296051025\n",
      "0.48356783390045166 0.9808030128479004\n",
      "0.4791640341281891 1.0168755054473877\n",
      "0.4897003173828125 1.0207637548446655\n",
      "0.49863559007644653 1.0532779693603516\n",
      "0.5136227011680603 1.0887072086334229\n",
      "0.5260562896728516 1.055066704750061\n",
      "0.4798029959201813 0.979474663734436\n",
      "0.4851192831993103 0.9835613965988159\n",
      "0.480072945356369 0.9574264883995056\n",
      "0.48287516832351685 0.955166757106781\n",
      "0.4651140570640564 0.9621219038963318\n",
      "0.49719497561454773 1.0233410596847534\n",
      "0.49038827419281006 0.9779917597770691\n",
      "0.5079302787780762 1.0430809259414673\n",
      "0.5086956024169922 1.032150387763977\n",
      "0.4912990629673004 1.0160670280456543\n",
      "0.47529858350753784 1.0034661293029785\n",
      "0.4871194064617157 0.9894798398017883\n",
      "0.5094892382621765 1.0081493854522705\n",
      "0.5105599164962769 1.0100654363632202\n",
      "0.4295421838760376 0.9336397051811218\n",
      "0.47472789883613586 0.9688690900802612\n",
      "0.4824424088001251 0.970072329044342\n",
      "0.5060633420944214 0.9930536150932312\n",
      "0.4838397204875946 0.9944044351577759\n",
      "0.47993215918540955 0.9967113137245178\n",
      "0.493008553981781 1.044244647026062\n",
      "0.4973144829273224 0.9980025291442871\n",
      "0.51213538646698 1.0086991786956787\n",
      "0.4436405599117279 0.9594584703445435\n",
      "0.49400147795677185 1.0281176567077637\n",
      "0.46936726570129395 0.9952684044837952\n",
      "0.4670678377151489 0.9641442894935608\n",
      "0.4946400225162506 1.0141428709030151\n",
      "0.47714728116989136 0.9795838594436646\n",
      "0.4692651033401489 0.9704895615577698\n",
      "0.4568714201450348 0.968535840511322\n",
      "0.4721187651157379 1.009159803390503\n",
      "0.4969952404499054 1.0462082624435425\n",
      "0.47655341029167175 0.99934983253479\n",
      "0.47598499059677124 1.0055222511291504\n",
      "0.45251932740211487 0.9317106008529663\n",
      "0.4582015872001648 1.0003442764282227\n",
      "0.48363327980041504 1.0006728172302246\n",
      "0.4673316180706024 1.0295878648757935\n",
      "0.4500088691711426 0.9442195892333984\n",
      "0.5111804604530334 1.033789038658142\n",
      "0.5083608031272888 1.0563960075378418\n",
      "0.4788629710674286 0.9695366024971008\n",
      "0.5062329769134521 1.0052430629730225\n",
      "0.4836410582065582 0.9707930684089661\n",
      "0.5210003852844238 1.0579257011413574\n",
      "0.4837662875652313 1.028214931488037\n",
      "0.486579030752182 0.9933544397354126\n",
      "0.5041285157203674 0.9697027802467346\n",
      "0.49013635516166687 1.035110592842102\n",
      "0.48418647050857544 1.0275440216064453\n",
      "0.47906890511512756 0.987196147441864\n",
      "0.5197094082832336 1.0419881343841553\n",
      "0.5083246827125549 1.072053074836731\n",
      "0.5132342576980591 1.0016217231750488\n",
      "0.5180562138557434 1.0506360530853271\n",
      "0.5172438025474548 1.0338950157165527\n",
      "0.5049538016319275 1.0163257122039795\n",
      "0.5030739307403564 1.0218234062194824\n",
      "0.47731220722198486 0.9700860381126404\n",
      "0.49086034297943115 1.0034210681915283\n",
      "0.4697031080722809 0.9409100413322449\n",
      "0.46890220046043396 0.9530282616615295\n",
      "0.4715312719345093 0.996309220790863\n",
      "0.4979357123374939 1.02907133102417\n",
      "0.4793778955936432 1.0276033878326416\n",
      "0.4987705647945404 1.0082896947860718\n",
      "0.5105078220367432 0.9924172759056091\n",
      "0.44477182626724243 0.9367982149124146\n",
      "0.4828885495662689 0.9913195967674255\n",
      "0.462616890668869 0.9803240895271301\n",
      "0.4803646504878998 1.0004202127456665\n",
      "0.4801624119281769 0.9716221690177917\n",
      "0.45824867486953735 0.9744300246238708\n",
      "0.47664496302604675 0.9923149943351746\n",
      "0.5283918976783752 1.0301073789596558\n",
      "0.4777500629425049 0.9625729322433472\n",
      "0.4692860245704651 0.9727174043655396\n",
      "0.44568371772766113 0.9411278367042542\n",
      "0.4876773953437805 0.9956884980201721\n",
      "0.47882288694381714 0.9743364453315735\n",
      "0.4764716625213623 0.9864843487739563\n",
      "0.4445526599884033 0.9614754915237427\n",
      "0.4580734968185425 0.9794243574142456\n",
      "0.46374914050102234 0.9728118181228638\n",
      "0.4948543608188629 1.0393450260162354\n",
      "0.47379201650619507 0.992560625076294\n",
      "0.5036891102790833 0.9877752065658569\n",
      "0.5014362931251526 1.0142687559127808\n",
      "0.4838815927505493 0.9706082940101624\n",
      "0.4744298756122589 1.0073163509368896\n",
      "0.5308627486228943 1.0517146587371826\n",
      "0.4876953065395355 1.0168923139572144\n",
      "0.4591805040836334 0.9912362098693848\n",
      "0.5006613731384277 0.9983457326889038\n",
      "0.5158244967460632 1.0151715278625488\n",
      "0.4612959623336792 0.9801262021064758\n",
      "0.45677438378334045 0.9895075559616089\n",
      "0.4852027893066406 1.016308307647705\n",
      "0.4903886020183563 1.0583773851394653\n",
      "0.4627895653247833 1.0046197175979614\n",
      "0.4555233120918274 0.9450666904449463\n",
      "0.4906812608242035 0.9772424697875977\n",
      "0.4977269768714905 1.0016887187957764\n",
      "0.5066351294517517 1.0035861730575562\n",
      "0.4865189492702484 1.0155096054077148\n",
      "0.5427382588386536 1.0662568807601929\n",
      "0.48970234394073486 1.0126733779907227\n",
      "0.5086780190467834 1.0236153602600098\n",
      "0.4881642758846283 0.9599871039390564\n",
      "0.5102975368499756 1.0530208349227905\n",
      "0.5001234412193298 1.045400619506836\n",
      "0.4784103333950043 0.9749953746795654\n",
      "0.4583006501197815 0.9512576460838318\n",
      "0.4878096282482147 1.0105749368667603\n",
      "0.4906446635723114 0.9852709770202637\n",
      "0.49201348423957825 0.9893283247947693\n"
     ]
    }
   ],
   "source": [
    "for _ in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        loss, *rest = rgl.loss(1000, device)\n",
    "        # loss = rgl.exact_loss(device)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # print(f'{loss.item()}')\n",
    "        print(f'{loss.item()} '+' '.join(f'{r.item()}' for r in rest))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "GaussianModel(r=-0.9999980330467224)"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgl.ir_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6631,  0.3182],\n",
      "        [-0.7455, -0.2374]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(rgl.get_mat())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2])\n",
      "tensor(3.6691, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1658, grad_fn=<MeanBackward0>)\n",
      "tensor(1.0000, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss, term = rgl.loss(10000, device)\n",
    "print(loss)\n",
    "print(term)\n",
    "print(rgl.exact_loss(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4934],\n",
      "        [-2.3520],\n",
      "        [-4.9539]])\n",
      "tensor([[-0.4934],\n",
      "        [-2.3520],\n",
      "        [-4.9539]])\n"
     ]
    }
   ],
   "source": [
    "test = rgl.get_ir_model()\n",
    "hmc = HMCSampler(test, [1,1])\n",
    "x = hmc.sample(device, samples=3).detach()\n",
    "E = test(x)\n",
    "print(x)\n",
    "print(x.roll(1,1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
